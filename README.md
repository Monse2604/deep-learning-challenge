# üöÄ **Deep Learning Model Report for Alphabet Soup**

## üìå **Project Overview**

This project aims to **predict whether organizations funded by Alphabet Soup** will be **successful** in using the funds effectively. The goal is to create a **binary classification model** using **deep learning** techniques to classify organizations as either **successful (1)** or **unsuccessful (0)** based on various features.

---

## üîç **Data Preprocessing**

The dataset consists of more than 34,000 organizations, and the preprocessing steps included:

- **Target Variable:**
  - `IS_SUCCESSFUL`: A binary variable indicating whether the organization successfully used the funding.

- **Feature Variables:**
  - Columns like `APPLICATION_TYPE`, `AFFILIATION`, `CLASSIFICATION`, `USE_CASE`, `ORGANIZATION`, `STATUS`, `INCOME_AMT`, `SPECIAL_CONSIDERATIONS`, `ASK_AMT` were used as features.

- **Data Cleanup:**
  - Removed irrelevant columns like `EIN` and `NAME` (identifiers).
  
- **Categorical Data:**
  - Categorical variables were encoded using `pd.get_dummies()`.
  - Rare categories were grouped into a new value "Other."

- **Data Scaling:**
  - The dataset was scaled using **`StandardScaler`** to improve model performance.

---

## üß† **Neural Network Model**

- **Architecture:**
  - **Input Layer:** The number of nodes corresponds to the number of features.
  - **Hidden Layers:** Two hidden layers using the `relu` activation function.
  - **Output Layer:** A single neuron with a `sigmoid` activation function for binary classification.

- **Model Evaluation:**
  - **Loss:** The model's loss was 0.6025.
  - **Accuracy:** The accuracy achieved on the test data was 72.46%.

- **Training Details:**
  - The model was trained for **100 epochs** with a batch size of **429**.
  - A **callback** was used to save the model weights every 5 epochs.

---

## üìà **Results**

- **Training and Evaluation Summary:**
  - During training, the model reached a **loss of 0.6025** and an **accuracy of 72.46%**.
  
- **Optimization Attempts:**
  - **Hidden layers:** More layers and neurons were added to improve the model's learning ability.
  - **Activation Functions:** Tweaked activation functions to achieve better non-linearity.

- **Model Performance:**
  - While the model did not meet the target accuracy of **75%**, it performed reasonably well for the dataset.

---

## üöÄ **Next Steps and Recommendations**

- **Future Optimizations:**
  - Further improvement could include experimenting with **deeper models**, **different activation functions**, or **regularization** techniques like **dropout**.

- **Alternative Models:**
  - **Random Forests** or **decision trees** could be explored for better interpretability and performance.

- **Performance Improvement:**
  - Fine-tuning the hyperparameters and using a larger training set could help achieve better accuracy.

---

## üìÇ **Model Export**

- The trained model was saved and exported as an HDF5 file named **`AlphabetSoupCharity.h5`** for future use and deployment.

---

## üìù **Project Files**

Key files in this project:

- **`AlphabetSoupCharity.h5`:** The trained deep learning model.

---

## ‚öôÔ∏è **How to Run the Code**

1. **Clone this repository:**
   ```bash
   git clone https://github.com/Monse2604/deep-learning-challenge.git
